task: ThorPickPlaceEnvHL
env_name: thor_pnp_hl
short_name: thor_pnp_hl
level: high
timesteps: 500_000
description: |
  Task description: 
  The task objective is to pick up all apples and eggs on the dining table and place them in the sink.
  In the task reward, the agent gets a reward of +10 after it successfully picks up an object and places it in the sink,
  and a step cost of -0.1 for each time step taken.
  The reward function you write does not need to encode the task objective. 

  Relevant task spaces:
  The agent's option/subtask (referred to as self.pnp_hl_actions in the code) space consists of picking up and placing the two types of objects.
  Each option takes multiple action steps to complete. 
  Taking a 'pick' option means that the agent will navigate to an object and pick it up.
  Taking a 'place' option means that the agent will navigate to the delivery location and place the object there.
  Note that the agent has to first go to the object to pick it up before placing the object.

  User preference:
  The agent should pick up an object type that's different from the previously placed object type, 
  as long as there are objects of the other type on the table need to be picked.

  Additional info: 
  You need to write a reward function to encode this user preference.
  The preference function you write will be used together with the task reward to train the agent.
  It can take up to 30 steps to reach an object and pick it up, or to reach the sink and drop it off.
  Make sure your reward scaling gives the preference for alternating objects much more weight than the negative step rewards, 
  but still lower than the positive task reward.
  Please make sure NOT to make the reward function stateful (i.e. you should not use function attributes or global variables.)
  You should also not write any other helper functions.
